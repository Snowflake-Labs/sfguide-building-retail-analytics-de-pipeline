{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68897282-4ed2-4656-ace4-41eb083475df",
   "metadata": {
    "collapsed": false,
    "name": "DataPipelineFlow"
   },
   "source": [
    "# Data Pipeline Flow\n",
    "\n",
    "## 1. Data Integration Layer\n",
    "### 1.1 MYSQL Source Integration\n",
    "- Implementation of `usp_loadMySQLData` stored procedure\n",
    "- Loading of product, customer, orders, inventory, and order items data  from MySQL Database\n",
    "- Using DB APIs\n",
    "\n",
    "### 1.2 File-based Data Integration\n",
    "- JSON data integration for product reviews\n",
    "- XML data integration for customer preferences and product specifications\n",
    "- Implementation of `usp_loadProductReviews_json` and `usp_loadCustomerPreferences_xml`\n",
    "\n",
    "## 2. Data Transformation Layer\n",
    "### 2.1 Bronze to Silver Transformation\n",
    "- Implementation of `usp_loadBronzeToSilver` stored procedure\n",
    "- Data cleaning and standardization\n",
    "- Telemetry integration for monitoring\n",
    "\n",
    "### 2.2 Data Validation\n",
    "- Integration with Great Expectations\n",
    "- Using Artifact Repository \n",
    "- Data quality checks and validation\n",
    "\n",
    "### 2.3 Silver to Gold Transformation\n",
    "- Implementation of `usp_loadSilverToGold` stored procedure\n",
    "- Business logic implementation\n",
    "- Performance optimization\n",
    "\n",
    "\n",
    "## 3. Analytics Layer\n",
    "\n",
    "### 3.1 Order Analytics\n",
    "- Implementation of `usp_loadOrderAnalytics` stored procedure\n",
    "- Creation of analytical tables for Streamlit dashboard\n",
    "- Business metrics and KPIs\n",
    "\n",
    "### 3.2 Customer Analytics\n",
    "- Implementation of `usp_load_customer_analytics` stored procedure\n",
    "- Creation of analytical tables for Streamlit dashboard\n",
    "\n",
    "\n",
    "## 4. Pipeline Orchestration\n",
    "### 4.1 Task DAG Implementation\n",
    "- Creation of `usp_createTaskDAG` stored procedure\n",
    "- Task dependencies and scheduling\n",
    "- Pipeline monitoring and history\n",
    "\n",
    "## 5. Dashboard\n",
    "- Implementation of `usp_saveChartsToStage` stored procedure\n",
    "- Chart generation and storage\n",
    "- Integration with Streamlit dashboard\n",
    "\n",
    "\n",
    "## 6. Cleanup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "session_init"
   },
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "\n",
    "# We can also use Snowpark for our analyses!\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "collapsed": false,
    "name": "pre_req"
   },
   "source": [
    ">Note:  **Ensure you copy all the py file from git repo to CODEFILES stage created in the setup section**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48252e16-4fcc-47d8-a7d8-fd0268b1fa5d",
   "metadata": {
    "collapsed": false,
    "name": "One_ingestion_layer"
   },
   "source": [
    "## 1. Data Integration Layer\n",
    "\n",
    "### 1.1 MYSQL Source Integration\n",
    "- Implementation of `usp_loadMySQLData` stored procedure\n",
    "- Loading of product, customer, orders, inventory, and order items data  from MySQL Database\n",
    "- Using DB APIs\n",
    "\n",
    "### 1.2 File-based Data Integration\n",
    "- JSON data integration for product reviews\n",
    "- XML data integration for customer preferences and product specifications\n",
    "- Implementation of `usp_loadProductReviews_json` and `usp_loadCustomerPreferences_xml`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab2a64d-9cf6-4721-af4f-474b2d7776f4",
   "metadata": {
    "collapsed": false,
    "name": "mysql_ingestion"
   },
   "source": [
    "### 1.1 MYSQL Source Integration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7f5138-94c2-4d2b-bf8f-ad4a4616c49d",
   "metadata": {
    "language": "sql",
    "name": "sp_loadMySQLTables"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE PROCEDURE usp_loadMySQLData(\n",
    "   source varchar \n",
    "   ,source_table varchar\n",
    "   ,dest_table_name varchar\n",
    ")\n",
    "RETURNS VARCHAR \n",
    "LANGUAGE PYTHON\n",
    "RUNTIME_VERSION = '3.10' \n",
    "ARTIFACT_REPOSITORY = snowflake.snowpark.pypi_shared_repository\n",
    "artifact_repository_packages=('snowflake-snowpark-python[pandas]','pymysql','snowflake-telemetry-python')\n",
    "IMPORTS = ('@CODEFILES/data_integration_for_databases.py','@CODEFILES/db_connections.py')\n",
    "-- PACKAGES = ('snowflake-telemetry-python') -- Include necessary packages\n",
    "HANDLER = 'data_integration_for_databases.main'\n",
    "EXTERNAL_ACCESS_INTEGRATIONS = (DBMS_ACCESS_INTEGRATION)\n",
    "SECRETS = ('mysql_cred'=MYSQL_DB_SECRET,'mysql_hostname'=MYSQL_HOSTNAME)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e16869f-4c55-41f0-8302-2140e6efb4fa",
   "metadata": {
    "collapsed": false,
    "name": "note"
   },
   "source": [
    "To execute the SP you can run the following query:  \\n\n",
    "\n",
    "For Python - session.call(\"usp_loadMySQLData\",'mysql','products','BRONZE_PRODUCTS') \n",
    "\n",
    "OR\n",
    "\n",
    "For SQL - CALL usp_loadMySQLData('mysql','products','BRONZE_PRODUCTS')\n",
    "\n",
    "\n",
    "Please note, we will be calling these SP's from the DAG that we will be created at the end of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac752b9-2e56-4c3d-bad9-67d335466320",
   "metadata": {
    "language": "sql",
    "name": "Run_usp_loadMySQLData"
   },
   "outputs": [],
   "source": [
    "-- session.call(\"usp_loadMySQLData\",'mysql','products','BRONZE_PRODUCTS')\n",
    "--  To test run the following query\n",
    " CALL usp_loadMySQLData('mysql','inventory','BRONZE_INVENTORY');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910693e4-1c73-47bf-a1a2-c66b8008003e",
   "metadata": {
    "collapsed": false,
    "name": "json_xml_ingestion"
   },
   "source": [
    "### 1.2 File-based Data Integration\n",
    "\n",
    "Ensure you can copied the data_integration_for_files.py from your clone repo to CODEFILES stage that you have created as part of the setup step. Also ensure you have copied the JSON and XML files from the cloned repo to SOURCE_FILES stage you have created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ebd646-41f4-495a-8ba6-dd1282554249",
   "metadata": {
    "language": "python",
    "name": "sp_loadJSON"
   },
   "outputs": [],
   "source": [
    "# Create a stored procedure to load product reviews from JSON to bronze layer\n",
    "# This procedure uses the DataIntegration class to parse and load JSON data\n",
    "\n",
    "# Import required modules for data integration and Snowpark functionality\n",
    "\n",
    "from snowflake.snowpark.functions import sproc\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake import telemetry\n",
    "import logging\n",
    "\n",
    "\n",
    "@sproc(session=session,\n",
    "       name=\"usp_loadProductReviews_json\", \n",
    "       replace=True, \n",
    "       is_permanent=True, \n",
    "       stage_location='@CODEFILES',\n",
    "       imports=[\"@CODEFILES/data_integration_for_files.py\"],\n",
    "       packages=[\"snowflake-snowpark-python\", \"pandas\",'python-dotenv',\"snowflake-telemetry-python\"])\n",
    "def loadProductReviews(session: Session) -> None:\n",
    "    from data_integration_for_files import DataIntegration\n",
    "    # Initialize data integration handler\n",
    "    data_ingestion = DataIntegration(session)\n",
    "    \n",
    "    # Log the start of data loading process\n",
    "    logging.info(\"Loading product reviews data\")\n",
    "    \n",
    "    # Call the JSON loading function to parse and load data to bronze layer\n",
    "    # The function handles JSON parsing, data transformation and loading to BRONZE_PRODUCT_REVIEWS table\n",
    "    telemetry.set_span_attribute(\"usp_loadProductReviews_json\", \"begin\")\n",
    "    telemetry.add_event(\"event_with_attributes\", {\"source_data\": \"Product Reviews JSON\"})\n",
    "    data_ingestion.load_product_reviews_to_bronze('@SOURCE_FILES/product_reviews.json')\n",
    "    \n",
    "    # Log successful completion of data loading\n",
    "    logging.info(\"Product reviews data loaded\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778e1317-ce82-4d08-8f92-9b8a5d11fe6b",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "run_usp_loadProductReviews_json"
   },
   "outputs": [],
   "source": [
    "call usp_loadProductReviews_json();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3354fdc6-7986-40a2-ac3d-13ff3dd2ea00",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "sp_loadCustPref_XML"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Import required modules for data integration and Snowpark functionality\n",
    "# This is loading the xml file which has the customer preferences and product specs.\n",
    "\n",
    "\n",
    "\n",
    "# Create a stored procedure to load customer preferences from XML to bronze layer\n",
    "# This procedure uses the DataIntegration class to parse and load XML data\n",
    "@sproc(name=\"usp_loadCustomerPreferences_xml\", replace=True, is_permanent=True, stage_location='@CODEFILES',\n",
    "       imports=[\"@CODEFILES/data_integration_for_files.py\"],\n",
    "       packages=[\"snowflake-snowpark-python==1.31.1\", \"pandas\",\"snowflake-telemetry-python\"])\n",
    "def loadCustomerPreferences(session: Session) -> None:\n",
    "    from data_integration_for_files import DataIntegration\n",
    "    from snowflake.snowpark.functions import sproc\n",
    "    from snowflake.snowpark import Session\n",
    "    session._use_scoped_temp_objects = False\n",
    "    from snowflake import telemetry\n",
    "    \n",
    "    # Initialize data integration handler\n",
    "    # session._use_scoped_temp_objects = False\n",
    "    data_ingestion = DataIntegration(session)\n",
    "    \n",
    "    # The function handles XML parsing, data transformation and loading to BRONZE_CUSTOMER_PREFERENCES table\n",
    "    telemetry.set_span_attribute(\"usp_loadCustomerPreferences_xml\", \"begin\")\n",
    "    telemetry.add_event(\"event_with_attributes\", {\"source_data\": \" XML Files\"})\n",
    "    data_ingestion.load_cust_pref_xml_to_bronze('@SOURCE_FILES/customer_preferences.xml','BRONZE_CUSTOMER_PREFERENCES')\n",
    "    \n",
    "    # Log successful completion of data loading\n",
    "    logging.info(\"Customer preferences data loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb38b79-e472-4886-92f4-6f6e189dc813",
   "metadata": {
    "language": "sql",
    "name": "run_usp_loadCustomerPreferences_xml"
   },
   "outputs": [],
   "source": [
    "call usp_loadCustomerPreferences_xml()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542b82cf-7054-419a-b548-f5eccb1d52b1",
   "metadata": {
    "language": "python",
    "name": "sp_loadProductSpec_XML"
   },
   "outputs": [],
   "source": [
    "# Import required modules for data integration and Snowpark functionality\n",
    "# This is loading the xml file which has the customer preferences and product specs.\n",
    "\n",
    "from snowflake.snowpark.functions import sproc\n",
    "from snowflake.snowpark import Session\n",
    "import logging\n",
    "# Create a stored procedure to load customer preferences from XML to bronze layer\n",
    "# This procedure uses the DataIntegration class to parse and load XML data\n",
    "@sproc(name=\"usp_loadProductSpecification_xml\", replace=True, is_permanent=True, stage_location='@CODEFILES',\n",
    "       imports=[\"@CODEFILES/data_integration_for_files.py\"],\n",
    "       packages=[\"snowflake-snowpark-python==1.31.1\", \"pandas\",\"snowflake-telemetry-python\"])\n",
    "def loadProductSpecification(session: Session) -> None:\n",
    "    from data_integration_for_files import DataIntegration\n",
    "\n",
    "    session._use_scoped_temp_objects = False\n",
    "    from snowflake import telemetry\n",
    "    \n",
    "    # Initialize data integration handler\n",
    "    # session._use_scoped_temp_objects = False\n",
    "    data_ingestion = DataIntegration(session)\n",
    "    \n",
    "    # The function handles XML parsing, data transformation and loading to BRONZE_CUSTOMER_PREFERENCES table\n",
    "    telemetry.set_span_attribute(\"usp_loadProductSpecification_xml\", \"begin\")\n",
    "    telemetry.add_event(\"event_with_attributes\", {\"source_data\": \" Prodcut Spec XML Files\"})\n",
    "    data_ingestion.load_product_specs_xml_to_bronze('@SOURCE_FILES/product_specifications.xml','BRONZE_PRODUCT_SPECIFICATIONS')\n",
    "    \n",
    "    # Log successful completion of data loading\n",
    "    logging.info(\"Product Spec data loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae87aa1-5818-4519-b78f-894912cb7b2b",
   "metadata": {
    "language": "sql",
    "name": "run_usp_loadProductSpecification_xml"
   },
   "outputs": [],
   "source": [
    "call usp_loadProductSpecification_xml()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddcdadc-3686-45be-9c85-dfa55d80a7e3",
   "metadata": {
    "language": "sql",
    "name": "cell3"
   },
   "outputs": [],
   "source": [
    "select * from BRONZE_PRODUCT_SPECIFICATIONS limit 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8992b6-598a-4c75-a99f-cab9feccbdad",
   "metadata": {
    "collapsed": false,
    "name": "Two_data_transformation"
   },
   "source": [
    "## 2. Data Transformation Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd038802-f51b-4d26-bc14-dcc34b290892",
   "metadata": {
    "collapsed": false,
    "name": "bronze_to_silver_transformation"
   },
   "source": [
    "### 2.1 Bronze to Silver Transformation\n",
    "\n",
    "We are creating a SP which basically performing some transformation and loading the data from Bronze layer to Silver layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3f34ef-4021-4b63-a63c-3b413953cff7",
   "metadata": {
    "language": "python",
    "name": "sp_load_Bronze_to_Silver"
   },
   "outputs": [],
   "source": [
    "# Import required modules for data transformation and telemetry\n",
    "from snowflake.snowpark.functions import sproc\n",
    "from snowflake.snowpark import Session\n",
    "\n",
    "# Create a stored procedure to transform data from Bronze to Silver layer\n",
    "# This follows the Medallion architecture pattern for data transformation\n",
    "@sproc(name=\"usp_loadBronzeToSilver\", replace=True, is_permanent=True, stage_location='@CODEFILES',\n",
    "       imports=[\"@CODEFILES/transformations.py\"],\n",
    "       packages=[\"snowflake-snowpark-python\",\"snowflake-ml-python\",\"snowflake-telemetry-python\"])\n",
    "def transform_bronze_to_silver(session: Session) -> None:\n",
    "    from transformations import DataTransformations\n",
    "    from snowflake import telemetry\n",
    "    # Add telemetry for monitoring the transformation process\n",
    "    telemetry.set_span_attribute(\"usp_loadBronzeToSilver\", \"begin\")\n",
    "    telemetry.add_event(\"event_with_attributes\", {\"source_data\": \"multiple tables\", \"transformation\": \"bronze to silver\"})\n",
    "    data_transformer = DataTransformations(session,'snowpark')\n",
    "    data_transformer.bronze_to_silver_transformation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee42ccb-1495-4eb0-8923-54108398f027",
   "metadata": {
    "collapsed": false,
    "name": "calling_usp_loadBronzeToSilver"
   },
   "source": [
    "\n",
    "Do not run the SP manually unless you have loaded all the mysql tables into Snowflake else the SP execution will fail as it fetches data from all the tables that is loaded from the MySQL.\n",
    "\n",
    "#call usp_loadBronzeToSilver()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5def3cc9-e472-4f1a-be6c-ecfc53d4cb6b",
   "metadata": {
    "collapsed": false,
    "name": "data_validation"
   },
   "source": [
    "### 2.2 Data Validation\n",
    "\n",
    "We are using Great Expectations (GE) library to perform some basic data validation. Here we are using artifact repository feature which will get the package from pypi and we don't have to perform any manual step to install the GE package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3853e2-ed05-433a-a9a9-07dc6e5efb87",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "sql",
    "name": "sp_DataValidation"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE   PROCEDURE usp_generateValidationResults()\n",
    "  RETURNS VARCHAR\n",
    "  LANGUAGE PYTHON\n",
    "  RUNTIME_VERSION = 3.9\n",
    "  ARTIFACT_REPOSITORY = snowflake.snowpark.pypi_shared_repository\n",
    "  PACKAGES = ('snowflake-snowpark-python')\n",
    "  ARTIFACT_REPOSITORY_PACKAGES = ('great-expectations==0.15.14','pandas==2.1.4')\n",
    "  HANDLER = 'generateValidationResults'\n",
    "  AS\n",
    "$$\n",
    "from great_expectations.data_context.types.base import DataContextConfig, DatasourceConfig, S3StoreBackendDefaults\n",
    "from great_expectations.core.batch import BatchRequest, RuntimeBatchRequest\n",
    "from great_expectations.data_context import BaseDataContext\n",
    "from snowflake.snowpark.types import IntegerType, StringType, StructField,VariantType,StructType,BooleanType\n",
    "# from great_expectations.checkpoint.types.checkpoint_result import CheckpointResult\n",
    "from great_expectations.data_context import DataContext\n",
    "from great_expectations.data_context import BaseDataContext\n",
    "from great_expectations.data_context.types.base import DataContextConfig, DatasourceConfig, FilesystemStoreBackendDefaults\n",
    "from great_expectations.checkpoint import Checkpoint   \n",
    "from snowflake.snowpark import Session\n",
    "                     \n",
    "def generateValidationResults(session: Session) -> str:\n",
    "    \n",
    "    from pathlib import Path\n",
    "    import os ,sys ,json ,tarfile\n",
    "    \n",
    "    data_context_config = DataContextConfig(\n",
    "    datasources={\n",
    "        \"dataframe_datasource\": DatasourceConfig(\n",
    "            class_name=\"PandasDatasource\",\n",
    "            batch_kwargs_generators={\n",
    "                \"subdir_reader\": {\n",
    "                    \"class_name\": \"SubdirReaderBatchKwargsGenerator\",\n",
    "                    \"base_directory\": \"/tmp/great_expectation/\",\n",
    "                }\n",
    "            },\n",
    "        )\n",
    "    },\n",
    "    store_backend_defaults=FilesystemStoreBackendDefaults(root_directory=\"/tmp/great_expectation\"),\n",
    "    )\n",
    "    \n",
    "    # Creating the GE context here\n",
    "    context = BaseDataContext(project_config=data_context_config)\n",
    "    \n",
    "    # Providing the datasource details which here is the pandas DF. We define the actual DF in the batch request which is defined after creating the DS\n",
    "    \n",
    "    datasource_config = {\n",
    "    \"name\": \"pandas_dataframe_datasource\",\n",
    "    \"class_name\": \"Datasource\",\n",
    "    \"module_name\": \"great_expectations.datasource\",\n",
    "    \"execution_engine\": {\n",
    "        \"module_name\": \"great_expectations.execution_engine\",\n",
    "        \"class_name\": \"PandasExecutionEngine\",\n",
    "    },\n",
    "    \"data_connectors\": {\n",
    "        \"default_runtime_data_connector_name\": {\n",
    "            \"class_name\": \"RuntimeDataConnector\",\n",
    "            \"module_name\": \"great_expectations.datasource.data_connector\",\n",
    "            \"batch_identifiers\": [\"default_identifier_name\"],\n",
    "        },\n",
    "    },\n",
    "            }\n",
    "    con='done'\n",
    "\n",
    "    # Adding the DS to the context\n",
    "    context.add_datasource(**datasource_config)\n",
    "    \n",
    "    # Converting the Snowpark DF into Pandas DF.\n",
    "    df=session.sql(\"select top 2000 * from SILVER_PRODUCT_REVIEWS\").to_pandas()\n",
    "    \n",
    "    #Creating the batch request whivh will be used \n",
    "    batch_request = RuntimeBatchRequest(\n",
    "                                datasource_name=\"pandas_dataframe_datasource\",\n",
    "                                data_connector_name=\"default_runtime_data_connector_name\",\n",
    "                                data_asset_name=\"PandasData\",  # This can be anything that identifies this data_asset for you\n",
    "                                runtime_parameters={\"batch_data\": df},  # df is your dataframe, you have created above.\n",
    "                                batch_identifiers={\"default_identifier_name\": \"default_identifier\"},\n",
    "                                )\n",
    "    \n",
    "    # Creating the expecation suite\n",
    "    context.create_expectation_suite(\n",
    "    expectation_suite_name=\"pandas_expectation_suite\", overwrite_existing=True)\n",
    "    \n",
    "    # Creating the validator which takes the batch request and expectation suite name\n",
    "    validator = context.get_validator(\n",
    "        batch_request=batch_request, expectation_suite_name=\"pandas_expectation_suite\"\n",
    "    )\n",
    "    \n",
    "    #Creating the required expectation. You can also create custom expectations as well. You can add additional inbuilt expectations as per the requirement\n",
    "    validator.expect_column_values_to_be_in_set(\"VERIFIED_PURCHASE\",[\"FALSE\"])\n",
    "    validator.expect_column_min_to_be_between(\"HELPFUL_VOTES\",12,20)\n",
    "    \n",
    "    #Saving the expectation \n",
    "    validator.save_expectation_suite(discard_failed_expectations=False)\n",
    "    \n",
    "    # Creating the checkpoint without writing to the file system and running by passing the run time parameters\n",
    "\n",
    "    my_checkpoint_name = \"pandas_checkpoint\"\n",
    "    checkpoint_config = {\n",
    "                \"name\": my_checkpoint_name,\n",
    "                \"config_version\": 1.0,\n",
    "                \"class_name\": \"SimpleCheckpoint\",\n",
    "                \"run_name_template\": \"%Y%m%d-%H%M%S-my-pandas_run-name-template\",\n",
    "            }\n",
    "            \n",
    "    context.add_checkpoint(**checkpoint_config)\n",
    "\n",
    "    # run expectation_suite against Pandas dataframe\n",
    "    res = context.run_checkpoint(\n",
    "            checkpoint_name = my_checkpoint_name,\n",
    "            validations=[\n",
    "                {\n",
    "                    \"batch_request\": batch_request,\n",
    "                    \"expectation_suite_name\": \"pandas_expectation_suite\",\n",
    "                }\n",
    "            ],\n",
    "        )\n",
    "    \n",
    "        \n",
    "    # Defining the schema, creating the Snowpark DF and and writing the validation results to a table.\n",
    "    schema = StructType([StructField(\"RunStatus\", BooleanType()),StructField(\"RunId\", VariantType()), StructField(\"RunValidation\", VariantType())])\n",
    "\n",
    "    df=session.create_dataframe([[res.success, json.loads(str(res.run_id)),json.loads(str(res.list_validation_results()))]], schema)\n",
    "\n",
    "    df.write.mode('append').saveAsTable('GreatExpeactionValidationsResults')\n",
    "    return 'SUCCESS'\n",
    "$$;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd1c9cc-ecfd-4ef4-bd8e-54133dd2ec5e",
   "metadata": {
    "collapsed": false,
    "name": "silver_gold_transformation"
   },
   "source": [
    "### 2.3 Silver to Gold Transformation\n",
    "\n",
    "Here we are loading the aggregrated results into the Gold layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5a352f-b86d-4e53-a7ee-4caedef1c798",
   "metadata": {
    "language": "python",
    "name": "sp_load_Silver_To_Gold"
   },
   "outputs": [],
   "source": [
    "\n",
    "from snowflake import telemetry\n",
    "from snowflake.snowpark.functions import sproc\n",
    "from snowflake.snowpark import Session\n",
    "# session.add_import(\"transformations.py\")\n",
    "@sproc(name=\"usp_loadSilverToGold\", replace=True, is_permanent=True, stage_location='@CODEFILES',imports=[\"@CODEFILES/transformations.py\"]\n",
    "       ,packages=[\"snowflake-snowpark-python\",\"snowflake-ml-python\",\"snowflake-telemetry-python\"])\n",
    "def transform_silver_to_gold(session: Session) -> None:\n",
    "    from transformations import DataTransformations\n",
    "    telemetry.set_span_attribute(\"usp_loadSilverToGold\", \"begin\")\n",
    "    telemetry.add_event(\"event_with_attributes\", {\"source_data\": \"Silver tables\", \"transformation\": \"silver to gold\"})\n",
    "    data_transformer = DataTransformations(session,'snowpark')\n",
    "    data_transformer.silver_to_gold_transformation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f289c0-252d-439b-9024-87361aa5faa1",
   "metadata": {
    "collapsed": false,
    "name": "Three_Analytics_Layer"
   },
   "source": [
    "## 3. Analytics Layer\n",
    "\n",
    "### 3.1 Order Analytics\n",
    "- Implementation of `usp_loadOrderAnalytics` stored procedure\n",
    "- Creation of order analytical tables for Streamlit dashboard\n",
    "- Business metrics and KPIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c017287-c2ee-4afe-b1db-b07aba39210e",
   "metadata": {
    "language": "python",
    "name": "sp_loadOrderAnalytics"
   },
   "outputs": [],
   "source": [
    "from snowflake import telemetry\n",
    "from snowflake.snowpark.functions import sproc\n",
    "from snowflake.snowpark import Session\n",
    "# session.add_import(\"order_analytics.py\")\n",
    "@sproc(name=\"usp_loadOrderAnalytics\", replace=True, is_permanent=True, stage_location='@CODEFILES',imports=[\"@CODEFILES/order_analytics.py\"]\n",
    "       ,packages=[\"snowflake-snowpark-python\",\"pandas\",\"snowflake-ml-python\",\"snowflake-telemetry-python\"])\n",
    "def load_order_analytics(session: Session) -> None:\n",
    "    from order_analytics import OrderAnalytics,main\n",
    "    telemetry.set_span_attribute(\"usp_loadOrderAnalytics\", \"begin\")\n",
    "    telemetry.add_event(\"event_with_attributes\", {\"source_data\": \"Gold tables\", \"transformation\": \"order analytics\"})\n",
    "    main(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc97f9a7-fc06-4571-aaac-dac26fad3084",
   "metadata": {
    "collapsed": false,
    "name": "customer_analytics"
   },
   "source": [
    "### 3.2 Customer Analytics\n",
    "- Implementation of `usp_load_customer_analytics` stored procedure\n",
    "- Creation of analytical tables for Streamlit dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5557c48-7e0e-4a91-98de-f2bbc7d2a86a",
   "metadata": {
    "language": "sql",
    "name": "sp_load_customer_analytics"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE PROCEDURE usp_load_customer_analytics()\n",
    "RETURNS VARCHAR \n",
    "LANGUAGE PYTHON\n",
    "RUNTIME_VERSION = '3.10' \n",
    "ARTIFACT_REPOSITORY = snowflake.snowpark.pypi_shared_repository\n",
    "artifact_repository_packages=('snowflake-snowpark-python[modin]','pymysql','snowflake-telemetry-python')\n",
    "-- PACKAGES = ('snowflake-telemetry-python','snowflake-snowpark-python','modin','pymysql','snowflake-telemetry-python') -- Include necessary packages\n",
    "-- imports=[\"@CODEFILES/customer_analytics.py\",\"@CODEFILES/data_integration_for_analytics.py\"],\n",
    "IMPORTS = ('@CODEFILES/customer_analytics.py','@CODEFILES/data_integration_for_analytics.py','@CODEFILES/db_connections.py')\n",
    "HANDLER = 'customer_analytics.run'\n",
    "EXTERNAL_ACCESS_INTEGRATIONS = (DBMS_ACCESS_INTEGRATION)\n",
    "SECRETS = ('mysql_cred'=MYSQL_DB_SECRET,'mysql_hostname'=MYSQL_HOSTNAME)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdea30ff-5f99-4945-ad19-9a1dbb9d2613",
   "metadata": {
    "collapsed": false,
    "name": "Four_Pipeline_Orchestration"
   },
   "source": [
    "\n",
    "## 4. Pipeline Orchestration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45faef1f-f0d0-4786-8d52-5d5153fe1810",
   "metadata": {
    "collapsed": false,
    "name": "dag_implementation"
   },
   "source": [
    "### 4.1 Task DAG Implementation\n",
    "- Creation of `usp_createTaskDAG` stored procedure\n",
    "- Task dependencies and scheduling\n",
    "- Pipeline monitoring and history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdc3643-442f-4c83-b219-497dfde850ae",
   "metadata": {
    "language": "python",
    "name": "sp_createTaskDAG"
   },
   "outputs": [],
   "source": [
    "from snowflake import telemetry\n",
    "from snowflake.snowpark.functions import sproc\n",
    "from snowflake.snowpark import Session\n",
    "# session.add_import(\"order_analytics.py\")\n",
    "@sproc(name=\"usp_createTaskDAG\", replace=True, is_permanent=True, stage_location='@CODEFILES'\n",
    "       ,imports=[\"@CODEFILES/create_task_DAG.py\"],packages=[\"snowflake-snowpark-python\",\"snowflake-telemetry-python\"])\n",
    "def create_task_dag(session: Session) -> None:\n",
    "    from create_task_DAG import create_task_dag,resume_tasks\n",
    "    telemetry.set_span_attribute(\"usp_createTaskDAG\", \"begin\")\n",
    "    telemetry.add_event(\"event_with_attributes\", {\"source_data\": \"Gold tables and Analytics\",\"status\":\"create dag\"})\n",
    "    create_task_dag(session,'retail_root_task')\n",
    "    telemetry.add_event(\"event_with_attributes\", {\"source_data\": \"Gold tables and Analytics\",\"status\":\"resume dag\"})\n",
    "    resume_tasks(session,'retail_root_task')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c487896-5e8c-44e8-8343-60cca5b24681",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "run_usp_createTaskDAG"
   },
   "outputs": [],
   "source": [
    "session.call('usp_createTaskDAG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e969cb39-7b36-4f5f-9b29-b511eca98b50",
   "metadata": {
    "collapsed": false,
    "name": "cell1"
   },
   "source": [
    "Go to Tasks in Retail_Pipeline_DB database and run the graph manually before going to next step.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10709d10-bd65-4ed0-9bfa-14956329b198",
   "metadata": {
    "collapsed": false,
    "name": "streamlit_app"
   },
   "source": [
    "## 5. Dashboard\n",
    "- Creation of Streamlit App\n",
    "- Implementation of `usp_saveChartsToStage` stored procedure which is called from the streamlit app to save specific chart as html file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f7079f-2733-4c2e-8582-b4e0da8236d3",
   "metadata": {
    "language": "python",
    "name": "create_usp_saveChartsToStage"
   },
   "outputs": [],
   "source": [
    "\n",
    "from snowflake.snowpark.functions import sproc\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark.types import StringType\n",
    "@sproc(name=\"usp_saveChartsToStage\", replace=True, is_permanent=True, stage_location='@CODEFILES'\n",
    "       ,imports=[\"@CODEFILES/save_charts_to_stage.py\"],packages=[\"snowflake-snowpark-python\"],input_types=[StringType()])\n",
    "def save_charts_to_stage(session: Session,html_file_name:str) -> None:\n",
    "    from save_charts_to_stage import main\n",
    "    main(session,html_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052eb287-9d20-45c9-9542-5f129101e26c",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "udf_saveCharttoStage"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE FUNCTION UDF_SAVE_CUSTOMER_PREF_CHART_HTML(\"CSVFILE\" VARCHAR)\n",
    "RETURNS VARCHAR\n",
    "LANGUAGE PYTHON\n",
    "RUNTIME_VERSION = '3.9'\n",
    "PACKAGES = ('snowflake-snowpark-python','altair')\n",
    "HANDLER = 'read_write'\n",
    "AS '\n",
    "from snowflake.snowpark.files import SnowflakeFile\n",
    "\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "import sys\n",
    "import io\n",
    "\n",
    "  \n",
    "def read_write(csvfile):\n",
    "  dst = SnowflakeFile.open_new_result(\"w\")\n",
    "\n",
    "  with SnowflakeFile.open(csvfile, ''rb'') as f:\n",
    "    df = pd.read_csv(f)\n",
    "    df.columns = df.columns.str.lower()\n",
    "    \n",
    "    \n",
    "  bar_chart = alt.Chart(df).mark_bar().encode(\n",
    "      x=alt.X(''preference'', title=''preference'', sort=''-y''),\n",
    "      y=alt.Y(''count'', title=''count''),\n",
    "      color=''preference'',\n",
    "      tooltip=[''preference'', ''count'']\n",
    "  ).properties(\n",
    "      width=1200,\n",
    "      height=900\n",
    "  ).configure_axis(\n",
    "      labelFontSize=12,\n",
    "      titleFontSize=14\n",
    "  ).configure_legend(\n",
    "      titleFontSize=14,\n",
    "      labelFontSize=12\n",
    "  ).configure_title(\n",
    "      fontSize=16\n",
    "  )\n",
    "  # Save the chart to a string buffer\n",
    "  chart_buffer = io.StringIO()\n",
    "  bar_chart.save(chart_buffer, format=''html'')\n",
    "  chart_html = chart_buffer.getvalue()\n",
    "  dst.write(chart_html)\n",
    "  return dst\n",
    "\n",
    "\n",
    "';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf43e2fa-243a-4d98-9d5e-4b5efbb1d419",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "sql",
    "name": "create_streamlitApp"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE STREAMLIT RETAIL_ANALYTICS_DASHBOARD\n",
    "  FROM @CODEFILES\n",
    "  MAIN_FILE = 'dashboard.py'\n",
    "  QUERY_WAREHOUSE = retail_wh\n",
    "  ;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54ece25-c3ab-45bc-8d2b-ef1b8e4b36e2",
   "metadata": {
    "collapsed": false,
    "name": "comments_1"
   },
   "source": [
    "\n",
    "### After you create the Streamlit App from the Snowsight go to Streamlit option under Projects open the RETAIL_ANALYTICS_DASHBOARD streamlit app."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b064deb-755a-42e5-a6df-44294ee3162f",
   "metadata": {
    "collapsed": false,
    "name": "cleanup"
   },
   "source": [
    "## 6. Cleanup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7dd72d-c10b-4cb6-8c88-6359e9c92e7b",
   "metadata": {
    "language": "python",
    "name": "Drop_DAG"
   },
   "outputs": [],
   "source": [
    "# Dropping the Task Graph created\n",
    "\n",
    "root_task_name='retail_root_task'\n",
    "session.sql(f'alter task {root_task_name} suspend').collect()\n",
    "res= session.sql(f''' select name\n",
    "    from table(information_schema.task_dependents(task_name => '{root_task_name}', recursive => true))''').collect()\n",
    "for r in res:\n",
    "    session.sql(f'drop task {r.NAME}').collect()\n",
    "    print(f'Dropped Task {r.NAME}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d212827-fed3-42d0-af8d-31096b899639",
   "metadata": {
    "language": "sql",
    "name": "cell4"
   },
   "outputs": [],
   "source": [
    "select current_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ddefaa-14b0-4715-b7dc-e0c7631b0637",
   "metadata": {
    "language": "sql",
    "name": "CleanUP"
   },
   "outputs": [],
   "source": [
    "Drop DATABASE RETAIL_PIPELINE_DB;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de14a13f-7b3b-421c-992a-7075a346302b",
   "metadata": {
    "language": "sql",
    "name": "drop_eventtable_db"
   },
   "outputs": [],
   "source": [
    "--  Dropping the Event Table Database\n",
    "\n",
    "USE ROLE ACCOUNTADMIN;\n",
    "ALTER ACCOUNT UNSET EVENT_TABLE;\n",
    "DROP DATABASE central_log_trace_db;\n",
    "DROP ROLE DE_DEMO_ROLE;\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "authorEmail": "phani.raj@snowflake.com",
   "authorId": "222433930658",
   "authorName": "PRAJ",
   "lastEditTime": 1753296708650,
   "notebookId": "bhk6scwuvmosq2y7hkpr",
   "sessionId": "955f9ab9-9ce3-4907-96e9-3648a8d49bcc"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
